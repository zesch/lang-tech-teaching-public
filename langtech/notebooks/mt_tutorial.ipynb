{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! usr/bin/env python3\n",
    "# -*- coding : utf-8 -*-\n",
    "\n",
    "'''\n",
    "author: aggarwal\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import join\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = join('data', 'eng_deu.txt')\n",
    "\n",
    "parallel_corpora = pd.read_csv(filename, sep='\\t', header = None, names=['eng', 'deu'], encoding = \"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parallel_corpora.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_corpora['eng'] = parallel_corpora['eng'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "parallel_corpora['deu'] = parallel_corpora['deu'].str.replace('[{}]'.format(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_corpora.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_parallel_corpora = parallel_corpora[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take first 10000 and do train-test\n",
    "train, test = train_test_split(limited_parallel_corpora, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install keras\n",
    "\n",
    "!{sys.executable} -m pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tokenizer\n",
    "tokenizer_eng = Tokenizer()\n",
    "tokenizer_ger = Tokenizer()\n",
    "tokenizer_eng.fit_on_texts(list(limited_parallel_corpora['eng'].values))\n",
    "tokenizer_ger.fit_on_texts(list(limited_parallel_corpora['deu'].values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_eng.word_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab size\n",
    "\n",
    "'''\n",
    "+1 needed because if you use the pad_sequence to process the sequence, you will find the 0 is used as the padding value. \n",
    "In order to distinguish between PAD and UNKNOWN, keras use word_count+1 as the index of UNKNOWN.\n",
    "'''\n",
    "eng_vocab_size = len(tokenizer_eng.word_index) + 1\n",
    "ger_vocab_size = len(tokenizer_ger.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eng_vocab_size)\n",
    "print(ger_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequencing and padding\n",
    "max_length = 30\n",
    "\n",
    "# training set\n",
    "trainX = tokenizer_ger.texts_to_sequences(train['deu'].values)\n",
    "trainY = tokenizer_eng.texts_to_sequences(train['eng'].values)\n",
    "\n",
    "trainX = pad_sequences(trainX, maxlen=max_length, padding='post')\n",
    "trainY = pad_sequences(trainY, maxlen=max_length, padding='post')\n",
    "\n",
    "\n",
    "# validation set\n",
    "testX = tokenizer_ger.texts_to_sequences(test['deu'].values)\n",
    "testY = tokenizer_eng.texts_to_sequences(test['eng'].values)\n",
    "\n",
    "testX = pad_sequences(testX, maxlen=max_length, padding='post')\n",
    "testY = pad_sequences(testY, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "def encode_output(sequences, vocab_size):\n",
    "\tylist = list()\n",
    "\tfor sequence in sequences:\n",
    "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "\t\tylist.append(encoded)\n",
    "\ty = np.array(ylist)\n",
    "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "\treturn y\n",
    "\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "testY = encode_output(testY, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate model\n",
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "\tmodel.add(LSTM(n_units))\n",
    "\tmodel.add(RepeatVector(tar_timesteps))\n",
    "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = define_model(ger_vocab_size, eng_vocab_size, 30, 30, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=10, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sentence = test['deu'].values[0]\n",
    "\n",
    "print(source_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see predictions of first test sentence\n",
    "\n",
    "pridiction_probabilities = model.predict(testX[:1], verbose=0)[0]\n",
    "    \n",
    "print(pridiction_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word encoded intergers out of probability map\n",
    "\n",
    "integers = [np.argmax(vector) for vector in pridiction_probabilities]\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert integers into words\n",
    "eng_sentence = []\n",
    "for each_int in integers:\n",
    "\tfor word, index in tokenizer_eng.word_index.items():\n",
    "\t\tif index == each_int:\n",
    "\t\t\teng_sentence.append(word)\n",
    "\n",
    "translated_sentence = ' '.join(eng_sentence)\n",
    "\n",
    "print(translated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sentence = test['eng'].values[0]\n",
    "\n",
    "print(actual_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate blue scores\n",
    "bleu_score = corpus_bleu([[actual_sentence.split()]], [translated_sentence.split()])\n",
    "\n",
    "print(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = input('please write a German source sentence\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence_encoded = tokenizer_ger.texts_to_sequences([input_sentence])\n",
    "print(input_sentence_encoded)\n",
    "\n",
    "input_sentence_encoded = pad_sequences(input_sentence_encoded, maxlen=max_length, padding='post')\n",
    "print(input_sentence_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pridiction_probabilities_inp = model.predict(input_sentence_encoded[:1], verbose=0)[0]\n",
    "    \n",
    "print(pridiction_probabilities_inp)\n",
    "integers_inp = [np.argmax(vector) for vector in pridiction_probabilities_inp]\n",
    "\n",
    "print(integers_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert integers into words\n",
    "eng_sentence = []\n",
    "for each_int in integers_inp:\n",
    "\tfor word, index in tokenizer_eng.word_index.items():\n",
    "\t\tif index == each_int:\n",
    "\t\t\teng_sentence.append(word)\n",
    "\n",
    "translated_sentence = ' '.join(eng_sentence)\n",
    "\n",
    "print(translated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUTATION\n",
    "\n",
    "eval_file =  pd.read_csv(join(\"data\",\"eng_deu_evaluation.txt\"), sep='\\t', header = None, names=['eng', 'ger'], encoding = \"UTF-8\")\n",
    "\n",
    "# apply same preprocessing as above\n",
    "eval_file['eng'] = eval_file['eng'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "eval_file['ger'] = eval_file['ger'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "\n",
    "evalX = tokenizer_ger.texts_to_sequences(eval_file['ger'].values)\n",
    "evalX = pad_sequences(evalX, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_probabilities = model.predict(evalX, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integers = [[np.argmax(vector) for vector in array] for array in prediction_probabilities]\n",
    "\n",
    "predicted_sentences = []\n",
    "for array in integers:\n",
    "    eng_sentence = []\n",
    "    for each_int in array:\n",
    "        for word, index in tokenizer_eng.word_index.items():\n",
    "            if index == each_int:\n",
    "                eng_sentence.append(word)\n",
    "                \n",
    "    translated_sentence = ' '.join(eng_sentence)\n",
    "    predicted_sentences.append(translated_sentence)\n",
    "print(predicted_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gold sentences\n",
    "print(eval_file['eng'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score = corpus_bleu([sentence.split() for sentence in eval_file['eng'].values], [translated_sentence.split() for translated_sentence in predicted_sentences])\n",
    "print(bleu_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
