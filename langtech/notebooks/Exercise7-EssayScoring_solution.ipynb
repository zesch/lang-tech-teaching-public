{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from IPython.core.display import HTML\n", "HTML(\"<style>\" + open(\"style.css\").read() + \"</style>\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"headline\">\n", "Language Technology / Sprachtechnologie\n", "<br><br>\n", "Wintersemester 2020/2021\n", "</div>\n", "<br>\n", "<div class=\"description\">\n", "    \u00dcbung zum Thema <i id=\"topic\">\"Essay Scoring\"</i>\n", "    <br><br>\n", "    Deadline Abgabe: <i #id=\"submission\">Friday, 08.01.2021 (11:55 Uhr)</i>\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from keras.models import Sequential\n", "from keras import layers\n", "from keras.preprocessing.sequence import pad_sequences\n", "from keras.utils import np_utils\n", "import pandas as pd\n", "from sklearn import datasets, svm, tree, metrics\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.metrics import confusion_matrix\n", "from sklearn.metrics import accuracy_score\n", "from sklearn.metrics import cohen_kappa_score\n", "from sklearn.preprocessing import OneHotEncoder\n", "from sklearn.metrics import classification_report\n", "from sklearn.tree import DecisionTreeClassifier\n", "from keras.preprocessing.sequence import pad_sequences\n", "from keras.preprocessing.text import Tokenizer\n", "import sys\n", "import numpy as np\n", "import seaborn as sns\n", "from mpl_toolkits.mplot3d import Axes3D\n", "import nltk\n", "import re\n", "import matplotlib.pyplot as plt\n", "from os.path import join"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Warm-up"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "    <i class=\"task\">Task 7.1:</i> <br>\n", "</div>\n", "\n", "Which of the following statements are true?\n", "\n", "1. Character n-grams are more robust against grammatical errors than token n-grams.\n", "2. Length features are always good in essay scoring.\n", "3. Length features are more susceptible to cheating than n-gram features.\n", "4. Essay scoring is potentially unfair to very creative learners."]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["<strong style=\"color: blue\">L\u00f6sung: </strong>\n", "\n", "1. Not necessarily: character n-grams are robust against spelling variation. With grammatical errors, the problem is often not that the word does not exist, but that the wrong form is used.\n", "2. No, they are only a good indicator when essays are written under a time constraint.\n", "3. True. You can influence length easily (repeating the same words over and over). For n-gram features you will probably have to cover some correct content.\n", "4. True. If someone writes something we have never seen in the training data, the algorithm will probably not know what to do with it. If someone writes mainstream, chances are high we can score it more accurately."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Features and Machine Learning"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "    <i class=\"task\">Task 7.2:</i> <br>\n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "   <i class=\"subtask\">7.2.1</i> <i class=\"l1\">L1</i> <br>\n", "</div>\n", "Import the following data file. It contains a number of linguistic features for essay scoring determined on the first prompt of the ASAP data record. \n", "<br> Output the NrofTokens feature."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv(join('data','featureFileAsap1.tsv'), sep='\\t')"]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["<strong style=\"color: blue\">L\u00f6sung: </strong>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["solution"]}, "outputs": [], "source": ["f1 = df['NrofTokens'].values\n", "print(f1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can plot the individual features across the different total scores."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["label ='avgNumCharsToken'\n", "plt.figure(figsize=(20, 10))\n", "plt.rcParams.update({'font.size': 22})\n", "plt.title(label+' vs Scores')\n", "plt.xlabel(label)\n", "plt.ylabel('Density')\n", "data = df[df.outcome == 2]\n", "sns.distplot(data[label], hist = False, kde = True, label='Score 2')\n", "data = df[df.outcome == 4]\n", "sns.distplot(data[label], hist = False, kde = True, label='Score 4')\n", "data = df[df.outcome == 5]\n", "sns.distplot(data[label], hist = False, kde = True, label='Score 5')\n", "data = df[df.outcome == 6]\n", "sns.distplot(data[label], hist = False, kde = True, label='Score 6')\n", "data = df[df.outcome == 7]\n", "sns.distplot(data[label], hist = False, kde = True, label='Score 7')\n", "data = df[df.outcome == 8]\n", "sns.distplot(data[label], hist = False, kde = True, label='Score 8')\n", "data = df[df.outcome == 9]\n", "sns.distplot(data[label], hist = False, kde = True, label='Score 9')\n", "data = df[df.outcome == 10]\n", "sns.distplot(data[label], hist = False, kde = True, label='Score 10')\n", "data = df[df.outcome == 11]\n", "sns.distplot(data[label], hist = False, kde = True, label='Score 11')\n", "data = df[df.outcome == 12]\n", "sns.distplot(data[label], hist = False, kde = True, label='Score 12')\n", "\n", "plt.legend(prop={'size': 12})\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "   <i class=\"subtask\">7.2.2</i> <i class=\"l1\">L1</i> <br>\n", "</div>\n", "\n", "Compare the plots for the three features, NrofTokens (essay length), avgNumCharsToken (word length) and PronounRatioI (what proportion of pronouns is 'I'). Which feature separates the different essay levels particularly well and which does not?"]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["<strong style=\"color: blue\">L\u00f6sung: </strong>\n", "\n", "Essay length works particularly well, the pronoun feature does not work well."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "   <i class=\"subtask\">7.2.3</i> <i class=\"l2\">L2</i> <br>\n", "</div>\n", "\n", "Train a Decision Tree Model on the data with last week's code and evaluate."]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["<strong style=\"color: blue\">L\u00f6sung: </strong>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["solution"]}, "outputs": [], "source": ["x = df.iloc[:, 1:len(df.columns)-1]\n", "y = df.iloc[:, [len(df.columns)-1]]\n", "\n", "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n", "\n", "c_tree = DecisionTreeClassifier(max_depth=4)\n", "c_tree.fit(x_train, y_train)\n", "\n", "predicted = list(c_tree.predict(x_test))\n", "gold = list(y_test.loc[:, \"outcome\"])\n", "print(classification_report(gold,predicted))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "    <i class=\"task\">Task 7.3:</i> <br>\n", "</div>\n", "The following code reads the essays from the first prompt of the ASAP dataset and their corresponding scores."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv(join('data', 'prompt1.tsv'), sep='\\t')\n", "essays = df['text']\n", "y = df['score1'].values\n", "\n", "# Essays are randomly split into 75% training data nd 25% test data\n", "essays_train, essays_test, y_train, y_test = train_test_split(essays, y, test_size=0.25, random_state=1000)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We convert the texts into a matrix of token counts based on the words in the training data. <br>\n", "This is a vector that counts for each word in the train data how often it occurs in an essay:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["vectorizer = CountVectorizer()\n", "vectorizer.fit(essays_train)\n", "X_train = vectorizer.transform(essays_train)\n", "X_test  = vectorizer.transform(essays_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "   <i class=\"subtask\">7.3.1</i> <i class=\"l2\">L2</i> <br>\n", "</div>\n", "\n", "Print the shape of the train and test data. <br> How many essays do we have? How many features does each essay have?"]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["<strong style=\"color: blue\">L\u00f6sung: </strong>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["solution"]}, "outputs": [], "source": ["print(X_train.shape)\n", "print(X_test.shape)"]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["1337 training essays, 446 test essays, 13349 features"]}, {"cell_type": "markdown", "metadata": {}, "source": ["First we use a standard shallow learning classifier on these features"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["classifier = LogisticRegression()\n", "classifier.fit(X_train, y_train)\n", "score = classifier.score(X_test, y_test)\n", "pred = classifier.predict(X_test)\n", "print(\"Accuracy:\", score)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We also print a confusion matrix, with the lines being the true labels:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(confusion_matrix(y_test, pred))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we build our first neural model. <br> We add two layers after the input: the first one with 10 nodes, the second one with 6 nodes, one per possible class. We print the summary of the model."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = Sequential()\n", "input_dim = X_train.shape[1]\n", "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n", "model.add(layers.Dense(6, activation='softmax'))\n", "model.compile(loss='categorical_crossentropy', \n", "               optimizer='adam', \n", "               metrics=['accuracy'])\n", "model.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We transform the output value to vectors, instead of '3' We would like to have '[0,0,1,0,0,0]'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_test_orig = y_test\n", "y_train = np.interp(y_train, (1, 6), (0, 5))\n", "y_test = np.interp(y_test, (1, 6), (0, 5))\n", "y_train = np_utils.to_categorical(y_train)\n", "y_test = np_utils.to_categorical(y_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we do the actual training, we train 10 times on the training data."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model.fit(X_train, y_train,\n", "                     epochs=10,\n", "                     verbose=True,\n", "                     validation_data=(X_test, y_test),\n", "                     batch_size=10)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We evaluate the loss on the training data and on the validation data."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["loss, accuracy = model.evaluate(X_train, y_train, verbose=True)\n", "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n", "loss, accuracy = model.evaluate(X_test, y_test, verbose=True)\n", "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "   <i class=\"subtask\">7.3.2</i> <i class=\"l1\">L1</i> <br>\n", "</div>\n", "\n", "Observe how the performance develops. Can you explain what is going on here?"]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["<strong style=\"color: blue\">L\u00f6sung: </strong>\n", "\n", "We overfit on the training data, i.e. our training performance gets better, but we fit so specifically on the training data, that our performance on the test data does not improve anymore."]}, {"cell_type": "markdown", "metadata": {}, "source": ["___\n", "We now get the predictions on the test data and compute a confusion matrix."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pred = model.predict(X_test)\n", "predarray = []\n", "for p in pred:\n", "    predarray.append(p.argmax()+1)\n", "print(confusion_matrix(y_test_orig, predarray))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In the first net, we represented each essays with an feature vector of fixed length, a BOW model. <br>\n", "Now we represent our input as a sequence of tokens, each token is represented by its index."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tokenizer = Tokenizer()\n", "tokenizer.fit_on_texts(essays_train)\n", "\n", "X_train = tokenizer.texts_to_sequences(essays_train)\n", "X_test = tokenizer.texts_to_sequences(essays_test)\n", "\n", "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index                                                                                                   "]}, {"cell_type": "markdown", "metadata": {}, "source": ["With CountVectorizer, we had stacked vectors of word counts, and each vector was the same length (the size of the total corpus vocabulary). With Tokenizer, the resulting vectors equal the length of each text, and the numbers don\u2019t denote counts, but rather correspond to the word values from the dictionary tokenizer.word_index.\n", " <br> <br>\n", " ___\n", " We consider only the first 500 words per essay, if there are less words, we pad the remaining words with 0s.\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["maxlen = 500\n", "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n", "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n", "\n", "embedding_dim = 50\n", "model2 = Sequential()\n", "model2.add(layers.Embedding(input_dim=vocab_size, \n", "                           output_dim=embedding_dim, \n", "                           input_length=maxlen))\n", "model2.add(layers.Flatten())\n", "model2.add(layers.Dense(10, activation='relu'))\n", "model2.add(layers.Dense(6, activation='softmax'))\n", "model2.compile(optimizer='adam',\n", "              loss='categorical_crossentropy',\n", "              metrics=['accuracy'])\n", "model2.summary()\n", "\n", "history = model2.fit(X_train, y_train,\n", "                    epochs=20,\n", "                    verbose=True,\n", "                    validation_data=(X_test, y_test),\n", "                    batch_size=10)\n", "loss, accuracy = model2.evaluate(X_train, y_train, verbose=True)\n", "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n", "loss, accuracy = model2.evaluate(X_test, y_test, verbose=True)\n", "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n", "\n", "pred = model2.predict(X_test)\n", "predarray = []\n", "for p in pred:\n", "    predarray.append(p.argmax()+1)\n", "print(confusion_matrix(y_test_orig, predarray))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In the following variant of the model, we use pretrained word-embeddings."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def create_embedding_matrix(filepath, word_index, embedding_dim):\n", "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n", "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n", "\n", "    with open(filepath, encoding=\"utf-8\") as f:\n", "        for line in f:\n", "            word, *vector = line.split()\n", "            if word in word_index:\n", "                idx = word_index[word] \n", "                embedding_matrix[idx] = np.array(\n", "                    vector, dtype=np.float32)[:embedding_dim]\n", "    return embedding_matrix\n", "\n", "\n", "embedding_dim = 100\n", "embedding_matrix = create_embedding_matrix(\n", "     'path to glove file',\n", "     tokenizer.word_index, embedding_dim)\n", "\n", "\n", "model3 = Sequential()\n", "model3.add(layers.Embedding(vocab_size, embedding_dim, \n", "                           weights=[embedding_matrix], \n", "                           input_length=maxlen, \n", "                           trainable=False))\n", "model3.add(layers.Flatten())\n", "model3.add(layers.Dense(10, activation='relu'))\n", "model3.add(layers.Dense(6, activation='softmax'))\n", "model3.compile(optimizer='adam',\n", "              loss='categorical_crossentropy',\n", "              metrics=['accuracy'])\n", "model3.summary()\n", "history = model3.fit(X_train, y_train,\n", "                    epochs=20,\n", "                    verbose=True,\n", "                    validation_data=(X_test, y_test),\n", "                    batch_size=10)\n", "loss, accuracy = model3.evaluate(X_train, y_train, verbose=True)\n", "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n", "loss, accuracy = model3.evaluate(X_test, y_test, verbose=True)\n", "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n", "\n", "pred = model3.predict(X_test)\n", "predarray = []\n", "for p in pred:\n", "    predarray.append(p.argmax()+1)\n", "print(confusion_matrix(y_test_orig, predarray))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "   <i class=\"subtask\">7.3.3</i> <i class=\"l2\">L2</i> <br>\n", "</div>\n", "\n", "Try to play with some of the parameters: Should we use longer or shorter essays lengths? A different number of hidden units? An additional hidden layer?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Overall evaluation"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "    <i class=\"task\">Task 7.4:</i>  <br>\n", "</div>\n", "\n", "The following code omputes linearly and quadratically weighted kappa (Set weight to \u201elinear\u201c or \u201equadratic\u201c)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cohen_kappa_score(y1, y2, labels=None, weights=None, sample_weight=None) "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Given is the following confusion matrix. Lines are true values. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["array=[[20, 5, 10],[15, 20, 5],[10, 20, 10]]\n", "df_cm = pd.DataFrame(array, index = [i for i in \"012\"],\n", "                     columns = [i for i in \"012\"])\n", "plt.figure(figsize = (4,3))\n", "sns.heatmap(df_cm, annot=True,cmap=\"Blues\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "   <i class=\"subtask\">7.4.1</i> <i class=\"l1\">L1</i> <br>\n", "</div>\n", "\n", "Compute the accuracy and the kappa for the confusion matrix above."]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["<strong style=\"color: blue\">L\u00f6sung: </strong>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["solution"]}, "outputs": [], "source": ["gold0 = [0] * 35 + [1] * 40 + [2] * 40\n", "pred0 = [0] * 20 + [1] * 5 + [2] * 10 + [0] * 15 + [1] * 20 + [2] * 5 + [0] * 10 + [1] * 20 + [2] * 10\n", "\n", "print(confusion_matrix(gold0, pred0))\n", "print(\"Accuracy: \",accuracy_score(gold0, pred0))\n", "print(\"Kappa: \",cohen_kappa_score(gold0, pred0, weights='quadratic'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "   <i class=\"subtask\">7.4.2</i> <i class=\"l1\">L1</i> <br>\n", "</div>\n", "Can you find a different confusion matrix, which has the same label distribution in the gold standard, the same accuracy, but a better QWK (Quadratic Weighted Kappa)?"]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["<strong style=\"color: blue\">L\u00f6sung: </strong>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["solution"]}, "outputs": [], "source": ["gold1 = [0] * 35 + [1] * 40 + [2] * 40\n", "pred1 = [0] * 20 + [1] * 5 + [2] * 10 + [0] * 15 + [1] * 20 + [2] * 5 + [0] * 0 + [1] * 30 + [2] * 10\n", "\n", "print(confusion_matrix(gold1, pred1))\n", "print(\"Accuracy: \",accuracy_score(gold1, pred1, normalize=True, sample_weight=None))\n", "print(\"Kappa: \",cohen_kappa_score(gold1, pred1, weights='quadratic'))"]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["A better matrix has less entries in the extreme cases (0-2, 2-0) while having the same number of gold standard instances per class."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "   <i class=\"subtask\">7.4.3</i> <i class=\"l1\">L1</i> <br>\n", "</div>\n", "\n", "Can you find one with a worse QWK? "]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["<strong style=\"color: blue\">L\u00f6sung: </strong>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["solution"]}, "outputs": [], "source": ["gold2 = [0] * 35 + [1] * 40 + [2] * 40\n", "pred2 = [0] * 20 + [1] * 0 + [2] * 15 + [0] * 15 + [1] * 20 + [2] * 5 + [0] * 10 + [1] * 20 + [2] * 10\n", "\n", "print(confusion_matrix(gold2, pred2))\n", "print(\"Accuracy: \",accuracy_score(gold2, pred2, normalize=True, sample_weight=None))\n", "print(\"Kappa: \",cohen_kappa_score(gold2, pred2, weights='quadratic'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Homework"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### *Submission guidelines*\n", "* *The submission has to be done by a team of two people. **Individual submissions will not be graded**.*\n", "* *Please state the **name and matriculation number of all team members** in every submission **clearly**.*\n", "* *Only **one team member should submit** the homework. If more than one version of the same homework is submitted by accident (submitted by more than one group member), please reach out to a tutor **as soon as possible**. Otherwise, the first submitted homework will be graded.*\n", "* *The submission must be in a Jupyter Notebook format (.ipynb). Submissions in **other formats will not be graded**.*\n", "* *It is not necessary to also submit the part of the exercise discussed by the tutor, please only submit the homework part.*\n", "* *If pictures need to be submitted, it is allowed to hand them in in a zip folder, together with the notebook. They should be added to the notebook like this: ``![example1](examplepicture1.PNG)`` (without apostrophs in a Markdown-Cell).*"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "    <i class=\"task\">7.1.</i> :::10 Homework points:::\n", "</div>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The following code makes a one-hot feature vector out of an essay and classifies it. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["my_essay = 'this is an example essay.'\n", "X_testing = tokenizer.texts_to_sequences([my_essay])\n", "X_testing = pad_sequences(X_testing, padding='post', maxlen=maxlen)\n", "print(model2.predict(X_testing).argmax()+1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "   <i class=\"subtask\">7.1.1.</i> <br>\n", "</div>\n", "\n", "Try to write a text that gets very high scores and a text that receives very low scores."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "   <i class=\"subtask\">7.1.2.</i> <br>\n", "</div>\n", "\n", "Can you write something that does not make sense to a human but still receives high scores?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "   <i class=\"subtask\">7.1.3.</i> <br>\n", "</div>\n", "\n", "Can you write something that should be correct and receives low scores?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["____\n", "For each task, describe how you proceeded. <br><br><br>\n", "__Remember we are in prompt 1:__ <br>\n", "<br>*More and more people use computers, but not everyone agrees that this benefits society. Those who support advances in technology believe that computers have a positive effect on people. They teach hand-eye coordination, give people the ability to learn about faraway places and people, and even allow people to talk online with other people. Others have different ideas. Some experts are concerned that people are spending too much time on their computers and less time exercising, enjoying nature, and interacting with family and friends.*\n", "<br><br>\n", "*Write a letter to your local newspaper in which you state your opinion on the effects computers have on people. Persuade the readers to agree with you.*\n", "<br><br>\n", "You may have a look at the full scoring guidelines for this prompt."]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "nlp-env", "language": "python", "name": "nlp-env"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.6"}}, "nbformat": 4, "nbformat_minor": 2}